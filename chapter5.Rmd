# WEEK 5: Dimensionality reduction techniques

In this exercise I was using data sets of human development and gender inequality. See the details from [this link](http://hdr.undp.org/en/content/human-development-index-hdi).

```{r, message=FALSE, warning=FALSE}
# Set working directory:
setwd("C:/Users/mikniemi/OneDrive - University of Helsinki/OpenDataScience/IODS/IODS-project")

# Access the necessary tool packages:
library(dplyr)
library(corrplot)
library(GGally)
library(FactoMineR)
library(ggplot2)
library(tidyr)

# Load and explore the exercise data:
human <- read.table("Data/human.txt")
summary(human)
ggpairs(human)
```

There are in total 8 variables explaining human development: 1) **Percentage of females having secondary education**, 2) **Percentage of females being employed**, 3) **Expected years of schooling**, 4) **Expected life**, 5) **The gross national income (GNI) Index**, 6) **Maternal mortality**, 7) **Adolescent birth**, and 8) **Female representation in parliament**. The figure above illustrates correlations between each pair of variables.

```{r}
# Compute the correlation matrix and visualize it with corrplot:
cor(human) %>% corrplot(type = "upper", tl.cex=0.6)
```

This figure presents well, that variables 'Labour.Female' and 'Woman_rep.Parliament' have low correlations with all other variables, but otherwise the variables are correlating with each other quite much.

### Principal Component Analysis (PCA) on the human data

```{r, warning=FALSE}
# Perform principal component analysis on the not standardized data (SVD method):
pca_human <- prcomp(human)

# Draw a biplot of the principal component representation and the original variables
biplot(pca_human, choices = 1:2, cex = c(0.5, 0.6), col = c("grey40", "deeppink2"), sub = "In the non-standardized data variable 'GNI' plays an overemphasized role because its huge scale compared to any other variable", cex.sub = 0.7, cex.axis = 0.8)
```

```{r}
# Standardize the variables:
human_std <- scale(human)

# Perform principal component analysis on the standardized data (SVD method):
pca_human_std <- prcomp(human_std)

# Draw a biplot of the principal component representation and the original variables:
biplot(pca_human_std, choices = 1:2, cex = c(0.5, 0.6), col = c("grey40", "deeppink2"), sub = "In the standardized data all the variables are more equal, and we can really utilize their variance for analyses", cex.sub = 0.7, cex.axis = 0.8)
```

Two figures above shows out how important it is to scale variables before principal component analysis. The latter figure illustrates also well the mutual correlations between some variables, as **in total six original features are contributing on the same dimension**, and therefore we could just pick up one of them to explain most variation of principal component PC1. Just two original features: *female labour and woman reps in parliament*, contribute on the vertical dimension and can explain variation of PC2.

```{r}
# Calculate summary of principal component analysis:
s <- summary(pca_human_std)

# Calculate and print out rounded percentages of variance captured by each PC:
pca_pr <- round(1*s$importance[2, ]*100, digits = 1)
pca_pr
```

Wow!! **57.0 % of the variation in the data could be explained by just one principal component**. By two components we could explain 72.6 % of the variation.

```{r}
# Let's adjust biplot by creating object 'pc_lab' to be used as axis labels:
pc_lab <- paste0(names(pca_pr), " (", pca_pr, " %)")

# draw a biplot
biplot(pca_human_std, cex = c(0.5, 0.6), col = c("grey40", "deeppink2"), cex.axis = 0.8, xlab = pc_lab[1], ylab = pc_lab[2])
```


### Multiple Correspondence Analysis (MCA) on the tea data

In contrast to PCA, in MCA we can use also **discrete variables**, even at *nominal scale*. For this exercise I load data 'tea' from the R package of 'FactoMineR'.

```{r, message=FALSE, warning=FALSE}
# Load the tea data:
data("tea")

# Look at the summary and structure of the data:
summary(tea)
str(tea)

# Select some variables that I'm interested in:
keep_columns <- c("Tea", "how", "home", "work", "price", "sex", "Sport", "age_Q")
tea_selection <- dplyr::select(tea, one_of(keep_columns))

# Visualize the data of the selected columns
gather(tea_selection) %>% ggplot(aes(value)) + facet_wrap("key", scales = "free") + geom_bar() + theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8))
```


```{r}
# Perform Multiple Correspondence Analysis and look at the summary of the model
mca <- MCA(tea_selection, graph = FALSE)
summary(mca)

# Visualize MCA
plot(mca, invisible=c("ind"), habillage = "quali")
```

The MCA summary tells that 10.7 % of the data variation could be explained by one dimension, 20.1 % by two dimensions, e.g... In total 17 dimensions are needed to cover all the data variation.
The MCA biplot shows the distances between different properties, and we can see that e.g. features 'p_cheap', '+60' and 'Not.home' are far from any other feature. Otherwise we can also see that e.g. people who use unpacked tea, are also ready to buy tea at upscale price.
Also we can see how well different dimensions can classify different features: e.g. Earl Grey drinkers can be separated from black and green tea drinkers by using dimension 1, but green and black tea drinkers are more difficult to recognize from each other using these two first dimensions.



